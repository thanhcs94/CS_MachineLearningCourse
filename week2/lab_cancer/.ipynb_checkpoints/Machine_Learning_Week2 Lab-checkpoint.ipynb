{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Lab: Classification, Parameter & Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's lab, we'll be focusing on implementing the following new supervised learning estimators:\n",
    "* Support Vector Machines\n",
    "* Random Forest Classifiers\n",
    "\n",
    "In addition, we'll look at techniques to optimize both the **parameter** values for our estimators, as well the **features** that we decide to use with them. We'll explore these concepts by using the following Python functions:\n",
    "* GridSearchCV\n",
    "* SelectFromModel\n",
    "* RFE (Recursive Feature Elimination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import the usual libraries, `pandas` as `pd` and `numpy` as `np`, and `matplotlib.pyplot` as `plt`. (Remember to also\n",
    "`%matplotlib inline`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "\n",
    "We'll use the built in breast cancer dataset from Scikit Learn: `load_breast_cancer`. We can `import` it `from sklearn.datasets`. The dataset can be loaded by calling `load_breast_cancer()`. Save it in a variable called `cancer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "# cancer = pd.read_csv('Breast_Cancer_dataset.csv')\n",
    "cancer = load_breast_cancer()\n",
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have worked with dataframes up until now, but this data set is presented in a dictionary form. Print out it's `keys`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can grab information and arrays out of this dictionary to set up our data frame and understanding of the features. The feature data is stored in `'data'`, the feature names are stored in `'feature_names'`, and the labels are stored in `target`. Print the `'feature_names'` to see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up DataFrame\n",
    "\n",
    "Now let's set up our dataset into a pandas dataframe. The `'data'` key from the `cancer` dictionary will be our feature data for our dataframe, and `'feature_names'` will be the columns. Store these features in `df_feat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat = pd.DataFrame(data = cancer['data'], columns = cancer['feature_names'])\n",
    "cancer['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the `head` of `df_feat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>15.470</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>22.880</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.37840</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>17.060</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.16540</td>\n",
       "      <td>0.36820</td>\n",
       "      <td>0.26780</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>15.490</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.17030</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>15.090</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.10500</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>19.190</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.14590</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.780</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>...</td>\n",
       "      <td>20.420</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.13960</td>\n",
       "      <td>0.56090</td>\n",
       "      <td>0.39650</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.170</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>...</td>\n",
       "      <td>20.960</td>\n",
       "      <td>29.94</td>\n",
       "      <td>151.70</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0.10370</td>\n",
       "      <td>0.39030</td>\n",
       "      <td>0.36390</td>\n",
       "      <td>0.17670</td>\n",
       "      <td>0.3176</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>...</td>\n",
       "      <td>16.840</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.19240</td>\n",
       "      <td>0.23220</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.730</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.080250</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.07682</td>\n",
       "      <td>...</td>\n",
       "      <td>15.030</td>\n",
       "      <td>32.01</td>\n",
       "      <td>108.80</td>\n",
       "      <td>697.7</td>\n",
       "      <td>0.16510</td>\n",
       "      <td>0.77250</td>\n",
       "      <td>0.69430</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.14310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.540</td>\n",
       "      <td>27.54</td>\n",
       "      <td>96.73</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.15950</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.073640</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.07077</td>\n",
       "      <td>...</td>\n",
       "      <td>17.460</td>\n",
       "      <td>37.13</td>\n",
       "      <td>124.10</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.65770</td>\n",
       "      <td>0.70260</td>\n",
       "      <td>0.17120</td>\n",
       "      <td>0.4218</td>\n",
       "      <td>0.13410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.680</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.073950</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.05922</td>\n",
       "      <td>...</td>\n",
       "      <td>19.070</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.14640</td>\n",
       "      <td>0.18710</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.08216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.130</td>\n",
       "      <td>20.68</td>\n",
       "      <td>108.10</td>\n",
       "      <td>798.8</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.20220</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.07356</td>\n",
       "      <td>...</td>\n",
       "      <td>20.960</td>\n",
       "      <td>31.48</td>\n",
       "      <td>136.80</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.17890</td>\n",
       "      <td>0.42330</td>\n",
       "      <td>0.47840</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.11420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.810</td>\n",
       "      <td>22.15</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05395</td>\n",
       "      <td>...</td>\n",
       "      <td>27.320</td>\n",
       "      <td>30.88</td>\n",
       "      <td>186.80</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>0.15120</td>\n",
       "      <td>0.31500</td>\n",
       "      <td>0.53720</td>\n",
       "      <td>0.23880</td>\n",
       "      <td>0.2768</td>\n",
       "      <td>0.07615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.05766</td>\n",
       "      <td>...</td>\n",
       "      <td>15.110</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.045680</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>...</td>\n",
       "      <td>14.500</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>...</td>\n",
       "      <td>10.230</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.097560</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>...</td>\n",
       "      <td>18.070</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.63050</td>\n",
       "      <td>0.23930</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.09946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.160</td>\n",
       "      <td>23.04</td>\n",
       "      <td>137.20</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.09428</td>\n",
       "      <td>0.10220</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.05278</td>\n",
       "      <td>...</td>\n",
       "      <td>29.170</td>\n",
       "      <td>35.59</td>\n",
       "      <td>188.00</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.31550</td>\n",
       "      <td>0.20090</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.07526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.650</td>\n",
       "      <td>21.38</td>\n",
       "      <td>110.00</td>\n",
       "      <td>904.6</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.06330</td>\n",
       "      <td>...</td>\n",
       "      <td>26.460</td>\n",
       "      <td>31.56</td>\n",
       "      <td>177.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>0.18050</td>\n",
       "      <td>0.35780</td>\n",
       "      <td>0.46950</td>\n",
       "      <td>0.20950</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.09564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.140</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>...</td>\n",
       "      <td>22.250</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.15450</td>\n",
       "      <td>0.39490</td>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.10590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.580</td>\n",
       "      <td>21.53</td>\n",
       "      <td>97.41</td>\n",
       "      <td>644.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.18680</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.087830</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.06924</td>\n",
       "      <td>...</td>\n",
       "      <td>17.620</td>\n",
       "      <td>33.21</td>\n",
       "      <td>122.40</td>\n",
       "      <td>896.9</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.66430</td>\n",
       "      <td>0.55390</td>\n",
       "      <td>0.27010</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.12750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.610</td>\n",
       "      <td>20.25</td>\n",
       "      <td>122.10</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.09440</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.077310</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.05699</td>\n",
       "      <td>...</td>\n",
       "      <td>21.310</td>\n",
       "      <td>27.26</td>\n",
       "      <td>139.90</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>0.13380</td>\n",
       "      <td>0.21170</td>\n",
       "      <td>0.34460</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.07421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.300</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>...</td>\n",
       "      <td>20.270</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.30</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.61100</td>\n",
       "      <td>0.63350</td>\n",
       "      <td>0.20240</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.570</td>\n",
       "      <td>15.05</td>\n",
       "      <td>115.00</td>\n",
       "      <td>955.1</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.11570</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>0.079530</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.06149</td>\n",
       "      <td>...</td>\n",
       "      <td>20.010</td>\n",
       "      <td>19.52</td>\n",
       "      <td>134.90</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.28120</td>\n",
       "      <td>0.24890</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.07919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>7.691</td>\n",
       "      <td>25.44</td>\n",
       "      <td>48.34</td>\n",
       "      <td>170.4</td>\n",
       "      <td>0.08668</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.092520</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>0.07751</td>\n",
       "      <td>...</td>\n",
       "      <td>8.678</td>\n",
       "      <td>31.89</td>\n",
       "      <td>54.49</td>\n",
       "      <td>223.6</td>\n",
       "      <td>0.15960</td>\n",
       "      <td>0.30640</td>\n",
       "      <td>0.33930</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.10660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>11.540</td>\n",
       "      <td>14.44</td>\n",
       "      <td>74.65</td>\n",
       "      <td>402.9</td>\n",
       "      <td>0.09984</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.067370</td>\n",
       "      <td>0.025940</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.06782</td>\n",
       "      <td>...</td>\n",
       "      <td>12.260</td>\n",
       "      <td>19.68</td>\n",
       "      <td>78.78</td>\n",
       "      <td>457.8</td>\n",
       "      <td>0.13450</td>\n",
       "      <td>0.21180</td>\n",
       "      <td>0.17970</td>\n",
       "      <td>0.06918</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.08134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>14.470</td>\n",
       "      <td>24.99</td>\n",
       "      <td>95.81</td>\n",
       "      <td>656.4</td>\n",
       "      <td>0.08837</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.06341</td>\n",
       "      <td>...</td>\n",
       "      <td>16.220</td>\n",
       "      <td>31.73</td>\n",
       "      <td>113.50</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.13400</td>\n",
       "      <td>0.42020</td>\n",
       "      <td>0.40400</td>\n",
       "      <td>0.12050</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>14.740</td>\n",
       "      <td>25.42</td>\n",
       "      <td>94.70</td>\n",
       "      <td>668.6</td>\n",
       "      <td>0.08275</td>\n",
       "      <td>0.07214</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.05680</td>\n",
       "      <td>...</td>\n",
       "      <td>16.510</td>\n",
       "      <td>32.29</td>\n",
       "      <td>107.40</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.13760</td>\n",
       "      <td>0.16110</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.2722</td>\n",
       "      <td>0.06956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>13.210</td>\n",
       "      <td>28.06</td>\n",
       "      <td>84.88</td>\n",
       "      <td>538.4</td>\n",
       "      <td>0.08671</td>\n",
       "      <td>0.06877</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.05781</td>\n",
       "      <td>...</td>\n",
       "      <td>14.370</td>\n",
       "      <td>37.17</td>\n",
       "      <td>92.48</td>\n",
       "      <td>629.6</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.13810</td>\n",
       "      <td>0.10620</td>\n",
       "      <td>0.07958</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.06443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>13.870</td>\n",
       "      <td>20.70</td>\n",
       "      <td>89.77</td>\n",
       "      <td>584.8</td>\n",
       "      <td>0.09578</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.023690</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.06688</td>\n",
       "      <td>...</td>\n",
       "      <td>15.050</td>\n",
       "      <td>24.75</td>\n",
       "      <td>99.17</td>\n",
       "      <td>688.6</td>\n",
       "      <td>0.12640</td>\n",
       "      <td>0.20370</td>\n",
       "      <td>0.13770</td>\n",
       "      <td>0.06845</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>0.08492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>13.620</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.05801</td>\n",
       "      <td>...</td>\n",
       "      <td>15.350</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.07174</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.06953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>10.320</td>\n",
       "      <td>16.35</td>\n",
       "      <td>65.31</td>\n",
       "      <td>324.9</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.06201</td>\n",
       "      <td>...</td>\n",
       "      <td>11.250</td>\n",
       "      <td>21.77</td>\n",
       "      <td>71.12</td>\n",
       "      <td>384.9</td>\n",
       "      <td>0.12850</td>\n",
       "      <td>0.08842</td>\n",
       "      <td>0.04384</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.2681</td>\n",
       "      <td>0.07399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>10.260</td>\n",
       "      <td>16.58</td>\n",
       "      <td>65.85</td>\n",
       "      <td>320.8</td>\n",
       "      <td>0.08877</td>\n",
       "      <td>0.08066</td>\n",
       "      <td>0.043580</td>\n",
       "      <td>0.024380</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06714</td>\n",
       "      <td>...</td>\n",
       "      <td>10.830</td>\n",
       "      <td>22.04</td>\n",
       "      <td>71.08</td>\n",
       "      <td>357.4</td>\n",
       "      <td>0.14610</td>\n",
       "      <td>0.22460</td>\n",
       "      <td>0.17830</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.2691</td>\n",
       "      <td>0.09479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>...</td>\n",
       "      <td>10.930</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.09350</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.07920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>10.820</td>\n",
       "      <td>24.21</td>\n",
       "      <td>68.89</td>\n",
       "      <td>361.6</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.06602</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>0.06328</td>\n",
       "      <td>...</td>\n",
       "      <td>13.030</td>\n",
       "      <td>31.45</td>\n",
       "      <td>83.90</td>\n",
       "      <td>505.6</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.16330</td>\n",
       "      <td>0.06194</td>\n",
       "      <td>0.03264</td>\n",
       "      <td>0.3059</td>\n",
       "      <td>0.07626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>10.860</td>\n",
       "      <td>21.48</td>\n",
       "      <td>68.51</td>\n",
       "      <td>360.5</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.04227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1661</td>\n",
       "      <td>0.05948</td>\n",
       "      <td>...</td>\n",
       "      <td>11.660</td>\n",
       "      <td>24.77</td>\n",
       "      <td>74.08</td>\n",
       "      <td>412.3</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>11.130</td>\n",
       "      <td>22.44</td>\n",
       "      <td>71.49</td>\n",
       "      <td>378.4</td>\n",
       "      <td>0.09566</td>\n",
       "      <td>0.08194</td>\n",
       "      <td>0.048240</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.06552</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020</td>\n",
       "      <td>28.26</td>\n",
       "      <td>77.80</td>\n",
       "      <td>436.6</td>\n",
       "      <td>0.10870</td>\n",
       "      <td>0.17820</td>\n",
       "      <td>0.15640</td>\n",
       "      <td>0.06413</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.08032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>12.770</td>\n",
       "      <td>29.43</td>\n",
       "      <td>81.35</td>\n",
       "      <td>507.9</td>\n",
       "      <td>0.08276</td>\n",
       "      <td>0.04234</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.05637</td>\n",
       "      <td>...</td>\n",
       "      <td>13.870</td>\n",
       "      <td>36.00</td>\n",
       "      <td>88.10</td>\n",
       "      <td>594.7</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.08653</td>\n",
       "      <td>0.06498</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>0.06484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>9.333</td>\n",
       "      <td>21.94</td>\n",
       "      <td>59.01</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.09240</td>\n",
       "      <td>0.05605</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.1692</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>...</td>\n",
       "      <td>9.845</td>\n",
       "      <td>25.05</td>\n",
       "      <td>62.86</td>\n",
       "      <td>295.8</td>\n",
       "      <td>0.11030</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.07993</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.07393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>12.880</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.061950</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>...</td>\n",
       "      <td>13.890</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.12270</td>\n",
       "      <td>0.16200</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.07242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>10.290</td>\n",
       "      <td>27.61</td>\n",
       "      <td>65.67</td>\n",
       "      <td>321.4</td>\n",
       "      <td>0.09030</td>\n",
       "      <td>0.07658</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.06127</td>\n",
       "      <td>...</td>\n",
       "      <td>10.840</td>\n",
       "      <td>34.91</td>\n",
       "      <td>69.57</td>\n",
       "      <td>357.6</td>\n",
       "      <td>0.13840</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.09127</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.08283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>10.160</td>\n",
       "      <td>19.59</td>\n",
       "      <td>64.73</td>\n",
       "      <td>311.7</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.07504</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.06331</td>\n",
       "      <td>...</td>\n",
       "      <td>10.650</td>\n",
       "      <td>22.88</td>\n",
       "      <td>67.88</td>\n",
       "      <td>347.3</td>\n",
       "      <td>0.12650</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.01005</td>\n",
       "      <td>0.02232</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.06742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>9.423</td>\n",
       "      <td>27.88</td>\n",
       "      <td>59.26</td>\n",
       "      <td>271.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.04971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>...</td>\n",
       "      <td>10.490</td>\n",
       "      <td>34.24</td>\n",
       "      <td>66.50</td>\n",
       "      <td>330.6</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.07158</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.06969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>14.590</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.06147</td>\n",
       "      <td>...</td>\n",
       "      <td>15.480</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.36620</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.08004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>...</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>14.050</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.06171</td>\n",
       "      <td>...</td>\n",
       "      <td>15.300</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>11.200</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.05502</td>\n",
       "      <td>...</td>\n",
       "      <td>11.920</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.220</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>...</td>\n",
       "      <td>17.520</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>20.920</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>0.06879</td>\n",
       "      <td>...</td>\n",
       "      <td>24.290</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.41860</td>\n",
       "      <td>0.65990</td>\n",
       "      <td>0.25420</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.09873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.560</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.41070</td>\n",
       "      <td>0.22160</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.130</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.097910</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>0.16280</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.600</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>0.053020</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.34030</td>\n",
       "      <td>0.14180</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.600</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.93870</td>\n",
       "      <td>0.26500</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.760</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0         17.990         10.38          122.80     1001.0          0.11840   \n",
       "1         20.570         17.77          132.90     1326.0          0.08474   \n",
       "2         19.690         21.25          130.00     1203.0          0.10960   \n",
       "3         11.420         20.38           77.58      386.1          0.14250   \n",
       "4         20.290         14.34          135.10     1297.0          0.10030   \n",
       "5         12.450         15.70           82.57      477.1          0.12780   \n",
       "6         18.250         19.98          119.60     1040.0          0.09463   \n",
       "7         13.710         20.83           90.20      577.9          0.11890   \n",
       "8         13.000         21.82           87.50      519.8          0.12730   \n",
       "9         12.460         24.04           83.97      475.9          0.11860   \n",
       "10        16.020         23.24          102.70      797.8          0.08206   \n",
       "11        15.780         17.89          103.60      781.0          0.09710   \n",
       "12        19.170         24.80          132.40     1123.0          0.09740   \n",
       "13        15.850         23.95          103.70      782.7          0.08401   \n",
       "14        13.730         22.61           93.60      578.3          0.11310   \n",
       "15        14.540         27.54           96.73      658.8          0.11390   \n",
       "16        14.680         20.13           94.74      684.5          0.09867   \n",
       "17        16.130         20.68          108.10      798.8          0.11700   \n",
       "18        19.810         22.15          130.00     1260.0          0.09831   \n",
       "19        13.540         14.36           87.46      566.3          0.09779   \n",
       "20        13.080         15.71           85.63      520.0          0.10750   \n",
       "21         9.504         12.44           60.34      273.9          0.10240   \n",
       "22        15.340         14.26          102.50      704.4          0.10730   \n",
       "23        21.160         23.04          137.20     1404.0          0.09428   \n",
       "24        16.650         21.38          110.00      904.6          0.11210   \n",
       "25        17.140         16.40          116.00      912.7          0.11860   \n",
       "26        14.580         21.53           97.41      644.8          0.10540   \n",
       "27        18.610         20.25          122.10     1094.0          0.09440   \n",
       "28        15.300         25.27          102.40      732.4          0.10820   \n",
       "29        17.570         15.05          115.00      955.1          0.09847   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "539        7.691         25.44           48.34      170.4          0.08668   \n",
       "540       11.540         14.44           74.65      402.9          0.09984   \n",
       "541       14.470         24.99           95.81      656.4          0.08837   \n",
       "542       14.740         25.42           94.70      668.6          0.08275   \n",
       "543       13.210         28.06           84.88      538.4          0.08671   \n",
       "544       13.870         20.70           89.77      584.8          0.09578   \n",
       "545       13.620         23.23           87.19      573.2          0.09246   \n",
       "546       10.320         16.35           65.31      324.9          0.09434   \n",
       "547       10.260         16.58           65.85      320.8          0.08877   \n",
       "548        9.683         19.34           61.05      285.7          0.08491   \n",
       "549       10.820         24.21           68.89      361.6          0.08192   \n",
       "550       10.860         21.48           68.51      360.5          0.07431   \n",
       "551       11.130         22.44           71.49      378.4          0.09566   \n",
       "552       12.770         29.43           81.35      507.9          0.08276   \n",
       "553        9.333         21.94           59.01      264.0          0.09240   \n",
       "554       12.880         28.92           82.50      514.3          0.08123   \n",
       "555       10.290         27.61           65.67      321.4          0.09030   \n",
       "556       10.160         19.59           64.73      311.7          0.10030   \n",
       "557        9.423         27.88           59.26      271.3          0.08123   \n",
       "558       14.590         22.68           96.39      657.1          0.08473   \n",
       "559       11.510         23.93           74.52      403.5          0.09261   \n",
       "560       14.050         27.15           91.38      600.4          0.09929   \n",
       "561       11.200         29.37           70.67      386.0          0.07449   \n",
       "562       15.220         30.62          103.40      716.9          0.10480   \n",
       "563       20.920         25.09          143.00     1347.0          0.10990   \n",
       "564       21.560         22.39          142.00     1479.0          0.11100   \n",
       "565       20.130         28.25          131.20     1261.0          0.09780   \n",
       "566       16.600         28.08          108.30      858.1          0.08455   \n",
       "567       20.600         29.33          140.10     1265.0          0.11780   \n",
       "568        7.760         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760        0.300100             0.147100         0.2419   \n",
       "1             0.07864        0.086900             0.070170         0.1812   \n",
       "2             0.15990        0.197400             0.127900         0.2069   \n",
       "3             0.28390        0.241400             0.105200         0.2597   \n",
       "4             0.13280        0.198000             0.104300         0.1809   \n",
       "5             0.17000        0.157800             0.080890         0.2087   \n",
       "6             0.10900        0.112700             0.074000         0.1794   \n",
       "7             0.16450        0.093660             0.059850         0.2196   \n",
       "8             0.19320        0.185900             0.093530         0.2350   \n",
       "9             0.23960        0.227300             0.085430         0.2030   \n",
       "10            0.06669        0.032990             0.033230         0.1528   \n",
       "11            0.12920        0.099540             0.066060         0.1842   \n",
       "12            0.24580        0.206500             0.111800         0.2397   \n",
       "13            0.10020        0.099380             0.053640         0.1847   \n",
       "14            0.22930        0.212800             0.080250         0.2069   \n",
       "15            0.15950        0.163900             0.073640         0.2303   \n",
       "16            0.07200        0.073950             0.052590         0.1586   \n",
       "17            0.20220        0.172200             0.102800         0.2164   \n",
       "18            0.10270        0.147900             0.094980         0.1582   \n",
       "19            0.08129        0.066640             0.047810         0.1885   \n",
       "20            0.12700        0.045680             0.031100         0.1967   \n",
       "21            0.06492        0.029560             0.020760         0.1815   \n",
       "22            0.21350        0.207700             0.097560         0.2521   \n",
       "23            0.10220        0.109700             0.086320         0.1769   \n",
       "24            0.14570        0.152500             0.091700         0.1995   \n",
       "25            0.22760        0.222900             0.140100         0.3040   \n",
       "26            0.18680        0.142500             0.087830         0.2252   \n",
       "27            0.10660        0.149000             0.077310         0.1697   \n",
       "28            0.16970        0.168300             0.087510         0.1926   \n",
       "29            0.11570        0.098750             0.079530         0.1739   \n",
       "..                ...             ...                  ...            ...   \n",
       "539           0.11990        0.092520             0.013640         0.2037   \n",
       "540           0.11200        0.067370             0.025940         0.1818   \n",
       "541           0.12300        0.100900             0.038900         0.1872   \n",
       "542           0.07214        0.041050             0.030270         0.1840   \n",
       "543           0.06877        0.029870             0.032750         0.1628   \n",
       "544           0.10180        0.036880             0.023690         0.1620   \n",
       "545           0.06747        0.029740             0.024430         0.1664   \n",
       "546           0.04994        0.010120             0.005495         0.1885   \n",
       "547           0.08066        0.043580             0.024380         0.1669   \n",
       "548           0.05030        0.023370             0.009615         0.1580   \n",
       "549           0.06602        0.015480             0.008160         0.1976   \n",
       "550           0.04227        0.000000             0.000000         0.1661   \n",
       "551           0.08194        0.048240             0.022570         0.2030   \n",
       "552           0.04234        0.019970             0.014990         0.1539   \n",
       "553           0.05605        0.039960             0.012820         0.1692   \n",
       "554           0.05824        0.061950             0.023430         0.1566   \n",
       "555           0.07658        0.059990             0.027380         0.1593   \n",
       "556           0.07504        0.005025             0.011160         0.1791   \n",
       "557           0.04971        0.000000             0.000000         0.1742   \n",
       "558           0.13300        0.102900             0.037360         0.1454   \n",
       "559           0.10210        0.111200             0.041050         0.1388   \n",
       "560           0.11260        0.044620             0.043040         0.1537   \n",
       "561           0.03558        0.000000             0.000000         0.1060   \n",
       "562           0.20870        0.255000             0.094290         0.2128   \n",
       "563           0.22360        0.317400             0.147400         0.2149   \n",
       "564           0.11590        0.243900             0.138900         0.1726   \n",
       "565           0.10340        0.144000             0.097910         0.1752   \n",
       "566           0.10230        0.092510             0.053020         0.1590   \n",
       "567           0.27700        0.351400             0.152000         0.2397   \n",
       "568           0.04362        0.000000             0.000000         0.1587   \n",
       "\n",
       "     mean fractal dimension           ...             worst radius  \\\n",
       "0                   0.07871           ...                   25.380   \n",
       "1                   0.05667           ...                   24.990   \n",
       "2                   0.05999           ...                   23.570   \n",
       "3                   0.09744           ...                   14.910   \n",
       "4                   0.05883           ...                   22.540   \n",
       "5                   0.07613           ...                   15.470   \n",
       "6                   0.05742           ...                   22.880   \n",
       "7                   0.07451           ...                   17.060   \n",
       "8                   0.07389           ...                   15.490   \n",
       "9                   0.08243           ...                   15.090   \n",
       "10                  0.05697           ...                   19.190   \n",
       "11                  0.06082           ...                   20.420   \n",
       "12                  0.07800           ...                   20.960   \n",
       "13                  0.05338           ...                   16.840   \n",
       "14                  0.07682           ...                   15.030   \n",
       "15                  0.07077           ...                   17.460   \n",
       "16                  0.05922           ...                   19.070   \n",
       "17                  0.07356           ...                   20.960   \n",
       "18                  0.05395           ...                   27.320   \n",
       "19                  0.05766           ...                   15.110   \n",
       "20                  0.06811           ...                   14.500   \n",
       "21                  0.06905           ...                   10.230   \n",
       "22                  0.07032           ...                   18.070   \n",
       "23                  0.05278           ...                   29.170   \n",
       "24                  0.06330           ...                   26.460   \n",
       "25                  0.07413           ...                   22.250   \n",
       "26                  0.06924           ...                   17.620   \n",
       "27                  0.05699           ...                   21.310   \n",
       "28                  0.06540           ...                   20.270   \n",
       "29                  0.06149           ...                   20.010   \n",
       "..                      ...           ...                      ...   \n",
       "539                 0.07751           ...                    8.678   \n",
       "540                 0.06782           ...                   12.260   \n",
       "541                 0.06341           ...                   16.220   \n",
       "542                 0.05680           ...                   16.510   \n",
       "543                 0.05781           ...                   14.370   \n",
       "544                 0.06688           ...                   15.050   \n",
       "545                 0.05801           ...                   15.350   \n",
       "546                 0.06201           ...                   11.250   \n",
       "547                 0.06714           ...                   10.830   \n",
       "548                 0.06235           ...                   10.930   \n",
       "549                 0.06328           ...                   13.030   \n",
       "550                 0.05948           ...                   11.660   \n",
       "551                 0.06552           ...                   12.020   \n",
       "552                 0.05637           ...                   13.870   \n",
       "553                 0.06576           ...                    9.845   \n",
       "554                 0.05708           ...                   13.890   \n",
       "555                 0.06127           ...                   10.840   \n",
       "556                 0.06331           ...                   10.650   \n",
       "557                 0.06059           ...                   10.490   \n",
       "558                 0.06147           ...                   15.480   \n",
       "559                 0.06570           ...                   12.480   \n",
       "560                 0.06171           ...                   15.300   \n",
       "561                 0.05502           ...                   11.920   \n",
       "562                 0.07152           ...                   17.520   \n",
       "563                 0.06879           ...                   24.290   \n",
       "564                 0.05623           ...                   25.450   \n",
       "565                 0.05533           ...                   23.690   \n",
       "566                 0.05648           ...                   18.980   \n",
       "567                 0.07016           ...                   25.740   \n",
       "568                 0.05884           ...                    9.456   \n",
       "\n",
       "     worst texture  worst perimeter  worst area  worst smoothness  \\\n",
       "0            17.33           184.60      2019.0           0.16220   \n",
       "1            23.41           158.80      1956.0           0.12380   \n",
       "2            25.53           152.50      1709.0           0.14440   \n",
       "3            26.50            98.87       567.7           0.20980   \n",
       "4            16.67           152.20      1575.0           0.13740   \n",
       "5            23.75           103.40       741.6           0.17910   \n",
       "6            27.66           153.20      1606.0           0.14420   \n",
       "7            28.14           110.60       897.0           0.16540   \n",
       "8            30.73           106.20       739.3           0.17030   \n",
       "9            40.68            97.65       711.4           0.18530   \n",
       "10           33.88           123.80      1150.0           0.11810   \n",
       "11           27.28           136.50      1299.0           0.13960   \n",
       "12           29.94           151.70      1332.0           0.10370   \n",
       "13           27.66           112.00       876.5           0.11310   \n",
       "14           32.01           108.80       697.7           0.16510   \n",
       "15           37.13           124.10       943.2           0.16780   \n",
       "16           30.88           123.40      1138.0           0.14640   \n",
       "17           31.48           136.80      1315.0           0.17890   \n",
       "18           30.88           186.80      2398.0           0.15120   \n",
       "19           19.26            99.70       711.2           0.14400   \n",
       "20           20.49            96.09       630.5           0.13120   \n",
       "21           15.66            65.13       314.9           0.13240   \n",
       "22           19.08           125.10       980.9           0.13900   \n",
       "23           35.59           188.00      2615.0           0.14010   \n",
       "24           31.56           177.00      2215.0           0.18050   \n",
       "25           21.40           152.40      1461.0           0.15450   \n",
       "26           33.21           122.40       896.9           0.15250   \n",
       "27           27.26           139.90      1403.0           0.13380   \n",
       "28           36.71           149.30      1269.0           0.16410   \n",
       "29           19.52           134.90      1227.0           0.12550   \n",
       "..             ...              ...         ...               ...   \n",
       "539          31.89            54.49       223.6           0.15960   \n",
       "540          19.68            78.78       457.8           0.13450   \n",
       "541          31.73           113.50       808.9           0.13400   \n",
       "542          32.29           107.40       826.4           0.10600   \n",
       "543          37.17            92.48       629.6           0.10720   \n",
       "544          24.75            99.17       688.6           0.12640   \n",
       "545          29.09            97.58       729.8           0.12160   \n",
       "546          21.77            71.12       384.9           0.12850   \n",
       "547          22.04            71.08       357.4           0.14610   \n",
       "548          25.59            69.10       364.2           0.11990   \n",
       "549          31.45            83.90       505.6           0.12040   \n",
       "550          24.77            74.08       412.3           0.10010   \n",
       "551          28.26            77.80       436.6           0.10870   \n",
       "552          36.00            88.10       594.7           0.12340   \n",
       "553          25.05            62.86       295.8           0.11030   \n",
       "554          35.74            88.84       595.7           0.12270   \n",
       "555          34.91            69.57       357.6           0.13840   \n",
       "556          22.88            67.88       347.3           0.12650   \n",
       "557          34.24            66.50       330.6           0.10730   \n",
       "558          27.27           105.90       733.5           0.10260   \n",
       "559          37.16            82.28       474.2           0.12980   \n",
       "560          33.17           100.20       706.7           0.12410   \n",
       "561          38.30            75.19       439.6           0.09267   \n",
       "562          42.79           128.70       915.0           0.14170   \n",
       "563          29.41           179.10      1819.0           0.14070   \n",
       "564          26.40           166.10      2027.0           0.14100   \n",
       "565          38.25           155.00      1731.0           0.11660   \n",
       "566          34.12           126.70      1124.0           0.11390   \n",
       "567          39.42           184.60      1821.0           0.16500   \n",
       "568          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     worst compactness  worst concavity  worst concave points  worst symmetry  \\\n",
       "0              0.66560          0.71190               0.26540          0.4601   \n",
       "1              0.18660          0.24160               0.18600          0.2750   \n",
       "2              0.42450          0.45040               0.24300          0.3613   \n",
       "3              0.86630          0.68690               0.25750          0.6638   \n",
       "4              0.20500          0.40000               0.16250          0.2364   \n",
       "5              0.52490          0.53550               0.17410          0.3985   \n",
       "6              0.25760          0.37840               0.19320          0.3063   \n",
       "7              0.36820          0.26780               0.15560          0.3196   \n",
       "8              0.54010          0.53900               0.20600          0.4378   \n",
       "9              1.05800          1.10500               0.22100          0.4366   \n",
       "10             0.15510          0.14590               0.09975          0.2948   \n",
       "11             0.56090          0.39650               0.18100          0.3792   \n",
       "12             0.39030          0.36390               0.17670          0.3176   \n",
       "13             0.19240          0.23220               0.11190          0.2809   \n",
       "14             0.77250          0.69430               0.22080          0.3596   \n",
       "15             0.65770          0.70260               0.17120          0.4218   \n",
       "16             0.18710          0.29140               0.16090          0.3029   \n",
       "17             0.42330          0.47840               0.20730          0.3706   \n",
       "18             0.31500          0.53720               0.23880          0.2768   \n",
       "19             0.17730          0.23900               0.12880          0.2977   \n",
       "20             0.27760          0.18900               0.07283          0.3184   \n",
       "21             0.11480          0.08867               0.06227          0.2450   \n",
       "22             0.59540          0.63050               0.23930          0.4667   \n",
       "23             0.26000          0.31550               0.20090          0.2822   \n",
       "24             0.35780          0.46950               0.20950          0.3613   \n",
       "25             0.39490          0.38530               0.25500          0.4066   \n",
       "26             0.66430          0.55390               0.27010          0.4264   \n",
       "27             0.21170          0.34460               0.14900          0.2341   \n",
       "28             0.61100          0.63350               0.20240          0.4027   \n",
       "29             0.28120          0.24890               0.14560          0.2756   \n",
       "..                 ...              ...                   ...             ...   \n",
       "539            0.30640          0.33930               0.05000          0.2790   \n",
       "540            0.21180          0.17970               0.06918          0.2329   \n",
       "541            0.42020          0.40400               0.12050          0.3187   \n",
       "542            0.13760          0.16110               0.10950          0.2722   \n",
       "543            0.13810          0.10620               0.07958          0.2473   \n",
       "544            0.20370          0.13770               0.06845          0.2249   \n",
       "545            0.15170          0.10490               0.07174          0.2642   \n",
       "546            0.08842          0.04384               0.02381          0.2681   \n",
       "547            0.22460          0.17830               0.08333          0.2691   \n",
       "548            0.09546          0.09350               0.03846          0.2552   \n",
       "549            0.16330          0.06194               0.03264          0.3059   \n",
       "550            0.07348          0.00000               0.00000          0.2458   \n",
       "551            0.17820          0.15640               0.06413          0.3169   \n",
       "552            0.10640          0.08653               0.06498          0.2407   \n",
       "553            0.08298          0.07993               0.02564          0.2435   \n",
       "554            0.16200          0.24390               0.06493          0.2372   \n",
       "555            0.17100          0.20000               0.09127          0.2226   \n",
       "556            0.12000          0.01005               0.02232          0.2262   \n",
       "557            0.07158          0.00000               0.00000          0.2475   \n",
       "558            0.31710          0.36620               0.11050          0.2258   \n",
       "559            0.25170          0.36300               0.09653          0.2112   \n",
       "560            0.22640          0.13260               0.10480          0.2250   \n",
       "561            0.05494          0.00000               0.00000          0.1566   \n",
       "562            0.79170          1.17000               0.23560          0.4089   \n",
       "563            0.41860          0.65990               0.25420          0.2929   \n",
       "564            0.21130          0.41070               0.22160          0.2060   \n",
       "565            0.19220          0.32150               0.16280          0.2572   \n",
       "566            0.30940          0.34030               0.14180          0.2218   \n",
       "567            0.86810          0.93870               0.26500          0.4087   \n",
       "568            0.06444          0.00000               0.00000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "5                    0.12440  \n",
       "6                    0.08368  \n",
       "7                    0.11510  \n",
       "8                    0.10720  \n",
       "9                    0.20750  \n",
       "10                   0.08452  \n",
       "11                   0.10480  \n",
       "12                   0.10230  \n",
       "13                   0.06287  \n",
       "14                   0.14310  \n",
       "15                   0.13410  \n",
       "16                   0.08216  \n",
       "17                   0.11420  \n",
       "18                   0.07615  \n",
       "19                   0.07259  \n",
       "20                   0.08183  \n",
       "21                   0.07773  \n",
       "22                   0.09946  \n",
       "23                   0.07526  \n",
       "24                   0.09564  \n",
       "25                   0.10590  \n",
       "26                   0.12750  \n",
       "27                   0.07421  \n",
       "28                   0.09876  \n",
       "29                   0.07919  \n",
       "..                       ...  \n",
       "539                  0.10660  \n",
       "540                  0.08134  \n",
       "541                  0.10230  \n",
       "542                  0.06956  \n",
       "543                  0.06443  \n",
       "544                  0.08492  \n",
       "545                  0.06953  \n",
       "546                  0.07399  \n",
       "547                  0.09479  \n",
       "548                  0.07920  \n",
       "549                  0.07626  \n",
       "550                  0.06592  \n",
       "551                  0.08032  \n",
       "552                  0.06484  \n",
       "553                  0.07393  \n",
       "554                  0.07242  \n",
       "555                  0.08283  \n",
       "556                  0.06742  \n",
       "557                  0.06969  \n",
       "558                  0.08004  \n",
       "559                  0.08732  \n",
       "560                  0.08321  \n",
       "561                  0.05905  \n",
       "562                  0.14090  \n",
       "563                  0.09873  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `target` key contains our labels (classes). Check it: you should see that it contains an array of 1s and 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the target in a labels dataframe. Call it `df_target`. The data is the `target` key, and we can call the columns `Cancer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.DataFrame(cancer['target'], columns = ['Cancer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `df_target`. You shoud see a very simple dataframe containing 0s (and 1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cancer\n",
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "Use `train_test_split` with `test_size=0.3` and `random_state=101`. We also want to convert our labels (`df_target`) from a column vector to a 1d array. Use the `np.ravel` function to do this! Do this when passing it as the `y` variable in `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target), test_size=0.33, random_state=102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Support Vector Classifier\n",
    "\n",
    "`from sklearn.svm import SVC`. Then, create a new `SVC()` instance, and save it in `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to `X_train` and `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and Evaluations\n",
    "\n",
    "Now let's predict using the trained model. Call `predict` on `X_test`, and print the `confusion_matrix` and the `classification_report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from nltk import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        70\n",
      "          1       0.63      1.00      0.77       118\n",
      "\n",
      "avg / total       0.39      0.63      0.48       188\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(X_test)\n",
    "classifi_report = classification_report(y_test, predict)\n",
    "print(classifi_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |   0   1 |\n",
      "--+---------+\n",
      "0 |  <.> 70 |\n",
      "1 |   .<118>|\n",
      "--+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confus_mat = ConfusionMatrix(list(y_test), list(predict))\n",
    "print(confus_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! You should see very low numbers: `0.38` for `precision`, `0.61` for `recall`, and `0.47` for `f1-score`. Notice that we are classifying everything into a single class (Class `1`)! This means our model needs to have it parameters adjusted (it may also help to normalize the data).\n",
    "\n",
    "We can search for parameters using a GridSearch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridsearchCV\n",
    "\n",
    "Finding the right parameters (like what `C` or `gamma` values to use) is a tricky task! But luckily, we can be a little lazy and just try a bunch of combinations and see what works best! This idea of creating a 'grid' of parameters and just trying out all the possible combinations is called a Gridsearch, this method is common enough that Scikit-learn has this functionality built in with GridSearchCV! The CV stands for cross-validation.\n",
    "\n",
    "GridSearchCV takes a dictionary that describes the parameters that should be tried and a model to train. The grid of parameters is defined as a dictionary, where the keys are the parameters and the values are the settings to be tested.  Use the following values for `param_grid`:\n",
    "* `'C': [0.1,1, 10, 100, 1000]`\n",
    "* `'gamma': [1,0.1,0.01,0.001,0.0001]`\n",
    "* `'kernel': ['rbf']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "  'C': [0.1,1, 10, 100, 1000],\n",
    "  'gamma': [1,0.1,0.01,0.001,0.0001],\n",
    "  'kernel': ['rbf'] \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the great things about GridSearchCV is that it is a meta-estimator. It takes an estimator like SVC, and creates a new estimator, that behaves exactly the same - in this case, like a classifier. You should add `refit=True` and choose verbose to whatever number you want (egs: `3`), higher the number, the more verbose (verbose just means the text output describing the process). `GridSearchCV` can be imported from `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gridSearch = GridSearchCV(model, param_grid,refit=True,verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What fit does is a bit more involved then usual. First, it runs the same loop with cross-validation, to find the best parameter combination. Once it has the best combination, it runs fit again on all data passed to fit (without cross-validation), to built a single new model using the best parameter setting. Call `fit` on your `grid` object on `X_train` and `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV]  C=0.1, gamma=1, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV]  C=0.1, gamma=1, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=0.1, gamma=0.1, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=0.1, gamma=0.1, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=0.1, gamma=0.01, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=0.1, gamma=0.01, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=0.1, gamma=0.01, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=0.1, gamma=0.001, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=0.1, gamma=0.001, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=0.1, gamma=0.001, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] . C=0.1, gamma=0.0001, kernel=rbf, score=0.8828125, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=0.1, gamma=0.0001, kernel=rbf, score=0.937007874015748, total=   0.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=0.1, gamma=0.0001, kernel=rbf, score=0.8968253968253969, total=   0.0s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, gamma=1, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV]  C=1, gamma=1, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV]  C=1, gamma=0.1, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV]  C=1, gamma=0.1, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV]  C=1, gamma=0.01, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV]  C=1, gamma=0.01, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] .... C=1, gamma=0.001, kernel=rbf, score=0.9296875, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.9606299212598425, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.9047619047619048, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] ... C=1, gamma=0.0001, kernel=rbf, score=0.9296875, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.9606299212598425, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.9047619047619048, total=   0.0s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV]  C=10, gamma=1, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV]  C=10, gamma=1, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV]  C=10, gamma=0.1, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV]  C=10, gamma=0.1, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] .... C=10, gamma=0.01, kernel=rbf, score=0.6328125, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV]  C=10, gamma=0.01, kernel=rbf, score=0.6456692913385826, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV]  C=10, gamma=0.01, kernel=rbf, score=0.6349206349206349, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .... C=10, gamma=0.001, kernel=rbf, score=0.921875, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.9448818897637795, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.9047619047619048, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] .. C=10, gamma=0.0001, kernel=rbf, score=0.9453125, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.9606299212598425, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.9206349206349206, total=   0.0s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV]  C=100, gamma=1, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV]  C=100, gamma=1, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=100, gamma=0.1, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV]  C=100, gamma=0.1, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ... C=100, gamma=0.01, kernel=rbf, score=0.6328125, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=100, gamma=0.01, kernel=rbf, score=0.6456692913385826, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV]  C=100, gamma=0.01, kernel=rbf, score=0.6349206349206349, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ... C=100, gamma=0.001, kernel=rbf, score=0.921875, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.9448818897637795, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.9047619047619048, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] .. C=100, gamma=0.0001, kernel=rbf, score=0.953125, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.9448818897637795, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.8968253968253969, total=   0.0s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV] ......... C=1000, gamma=1, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV]  C=1000, gamma=1, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV]  C=1000, gamma=1, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] ....... C=1000, gamma=0.1, kernel=rbf, score=0.625, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV]  C=1000, gamma=0.1, kernel=rbf, score=0.6299212598425197, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV]  C=1000, gamma=0.1, kernel=rbf, score=0.626984126984127, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV] .. C=1000, gamma=0.01, kernel=rbf, score=0.6328125, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV]  C=1000, gamma=0.01, kernel=rbf, score=0.6456692913385826, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV]  C=1000, gamma=0.01, kernel=rbf, score=0.6349206349206349, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] .. C=1000, gamma=0.001, kernel=rbf, score=0.921875, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.9448818897637795, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.9047619047619048, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.9453125, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.9448818897637795, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.8968253968253969, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=10)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridSearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the best parameters found by `GridSearchCV` in the `best_params_` attribute, and the best estimator in the `best_estimator_` attributes. Check them both...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridSearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! You should see that the new best parameters are `'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can re-run predictions on this grid object just like you would with a normal model. Run `predict` on `grid.best_estimator_` and then print the `confusion_matrix` and `classification_report` on these new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(gridSearch.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.93      0.94        70\n",
      "          1       0.96      0.97      0.96       118\n",
      "\n",
      "avg / total       0.95      0.95      0.95       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from nltk import ConfusionMatrix\n",
    "\n",
    "predict2 = gridSearch.best_estimator_.predict(X_test)\n",
    "classifi_report = classification_report(y_test, predict2)\n",
    "print(classifi_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see much better numbers! `0.95` for `precision`, `recall`, and `f1-score`. We used `GridSearchCV` to find the best parameter values for our Support Vector Classifier. Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the applications of machine learning techniques applied to health science and biomedical research. We'll use `RandomForestClassifier` and the Breast Cancer Wisconsin Dataset, downloaded from UC Irvine Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)).\n",
    "\n",
    "Read the `Breast_Cancer_dataset.csv` using `pd.read_csv`. Make sure to set `index_col='Id'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = pd.read_csv('Breast_Cancer_dataset.csv', index_col='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the `head` to make sure your dataset looks good. You should see 9 features, and a label column called `Class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump_Thickness</th>\n",
       "      <th>Uniformity_Cell_Size</th>\n",
       "      <th>Uniformity_Cell_Shape</th>\n",
       "      <th>Marginal_Adhesion</th>\n",
       "      <th>Single_Epithelial_Cell_Size</th>\n",
       "      <th>Bare_Nuclei</th>\n",
       "      <th>Bland_Chromatin</th>\n",
       "      <th>Normal_Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>431495</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324382</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167471</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213383</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225382</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Clump_Thickness  Uniformity_Cell_Size  Uniformity_Cell_Shape  \\\n",
       "Id                                                                      \n",
       "431495                 3                     1                      1   \n",
       "324382                 1                     1                      1   \n",
       "1167471                4                     1                      2   \n",
       "1213383                5                     1                      1   \n",
       "1225382                6                     2                      3   \n",
       "\n",
       "         Marginal_Adhesion  Single_Epithelial_Cell_Size  Bare_Nuclei  \\\n",
       "Id                                                                     \n",
       "431495                   1                            2            1   \n",
       "324382                   1                            2            1   \n",
       "1167471                  1                            2            1   \n",
       "1213383                  4                            2            1   \n",
       "1225382                  1                            2            1   \n",
       "\n",
       "         Bland_Chromatin  Normal_Nucleoli  Mitoses  Class  \n",
       "Id                                                         \n",
       "431495                 3                2        1      2  \n",
       "324382                 2                1        1      2  \n",
       "1167471                3                1        1      2  \n",
       "1213383                3                1        1      2  \n",
       "1225382                1                1        1      2  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your feature dataframe (`df_features`) and label dataframes (`df_labels`): drop the `'Class'` column to create the features, and use the `'Class'` column as your labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump_Thickness</th>\n",
       "      <th>Uniformity_Cell_Size</th>\n",
       "      <th>Uniformity_Cell_Shape</th>\n",
       "      <th>Marginal_Adhesion</th>\n",
       "      <th>Single_Epithelial_Cell_Size</th>\n",
       "      <th>Bare_Nuclei</th>\n",
       "      <th>Bland_Chromatin</th>\n",
       "      <th>Normal_Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>431495</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324382</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167471</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213383</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225382</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Clump_Thickness  Uniformity_Cell_Size  Uniformity_Cell_Shape  \\\n",
       "Id                                                                      \n",
       "431495                 3                     1                      1   \n",
       "324382                 1                     1                      1   \n",
       "1167471                4                     1                      2   \n",
       "1213383                5                     1                      1   \n",
       "1225382                6                     2                      3   \n",
       "\n",
       "         Marginal_Adhesion  Single_Epithelial_Cell_Size  Bare_Nuclei  \\\n",
       "Id                                                                     \n",
       "431495                   1                            2            1   \n",
       "324382                   1                            2            1   \n",
       "1167471                  1                            2            1   \n",
       "1213383                  4                            2            1   \n",
       "1225382                  1                            2            1   \n",
       "\n",
       "         Bland_Chromatin  Normal_Nucleoli  Mitoses  \n",
       "Id                                                  \n",
       "431495                 3                2        1  \n",
       "324382                 2                1        1  \n",
       "1167471                3                1        1  \n",
       "1213383                3                1        1  \n",
       "1225382                1                1        1  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels = cancer['Class']\n",
    "cancer.drop('Class', axis= 1, inplace = True)\n",
    "df_features = cancer\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump_Thickness</th>\n",
       "      <th>Uniformity_Cell_Size</th>\n",
       "      <th>Uniformity_Cell_Shape</th>\n",
       "      <th>Marginal_Adhesion</th>\n",
       "      <th>Single_Epithelial_Cell_Size</th>\n",
       "      <th>Bare_Nuclei</th>\n",
       "      <th>Bland_Chromatin</th>\n",
       "      <th>Normal_Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>431495</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324382</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167471</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213383</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225382</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Clump_Thickness  Uniformity_Cell_Size  Uniformity_Cell_Shape  \\\n",
       "Id                                                                      \n",
       "431495                 3                     1                      1   \n",
       "324382                 1                     1                      1   \n",
       "1167471                4                     1                      2   \n",
       "1213383                5                     1                      1   \n",
       "1225382                6                     2                      3   \n",
       "\n",
       "         Marginal_Adhesion  Single_Epithelial_Cell_Size  Bare_Nuclei  \\\n",
       "Id                                                                     \n",
       "431495                   1                            2            1   \n",
       "324382                   1                            2            1   \n",
       "1167471                  1                            2            1   \n",
       "1213383                  4                            2            1   \n",
       "1225382                  1                            2            1   \n",
       "\n",
       "         Bland_Chromatin  Normal_Nucleoli  Mitoses  \n",
       "Id                                                  \n",
       "431495                 3                2        1  \n",
       "324382                 2                1        1  \n",
       "1167471                3                1        1  \n",
       "1213383                3                1        1  \n",
       "1225382                1                1        1  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `train_test_split` to split your data. Let's use `test_size=0.25` this time.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, df_labels, test_size=0.25, random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `shape` of your training and testing features and labels (`X_train`, `X_test`, `y_train`, `y_test`). You should have 9 features, with 358 training instances, and 120 test instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358, 9)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "# X_test.shape\n",
    "# y_train.shape\n",
    "# y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build our `RandomForestClassifier` !  You can import it from `sklearn.ensemble`. Create an instance of it, call it `rfc`, and `fit` it to your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one cool thing about `RandomForestClassifier` is that, because of the way in which Random Forests work, they are able to assign an `importance` to every feature (which is exactly what it sounds like: a score / numerical value that represents how important every feature was, in coming up with the final decision). Let's take a look at the `feature_importances_` attribute.\n",
    "\n",
    "Create a DataFrame called `importances`. For your `data`, pass a dictionary object that contains 2 keys:\n",
    "* `'feature'`, which will be the features (i.e. the columns of your feature dataframe that you made earlier)\n",
    "* `'importance'`, which will be the `feature_importances_` attribute of your `RandomForestClassifier` object (`rfc`). Use `np.round` to set the number of decimal places to `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clump_Thickness</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uniformity_Cell_Size</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Uniformity_Cell_Shape</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marginal_Adhesion</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Single_Epithelial_Cell_Size</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bare_Nuclei</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bland_Chromatin</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Normal_Nucleoli</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mitoses</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature  importance\n",
       "0              Clump_Thickness        0.04\n",
       "1         Uniformity_Cell_Size        0.37\n",
       "2        Uniformity_Cell_Shape        0.23\n",
       "3            Marginal_Adhesion        0.01\n",
       "4  Single_Epithelial_Cell_Size        0.08\n",
       "5                  Bare_Nuclei        0.05\n",
       "6              Bland_Chromatin        0.13\n",
       "7              Normal_Nucleoli        0.08\n",
       "8                      Mitoses        0.00"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.feature_importances_\n",
    "importances = pd.DataFrame({\"feature\":df_features.columns, \"importance\":rfc.feature_importances_})\n",
    "importances.importance = importances.importance.round(2) \n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, sort the importances dataframe in descending value. You can do this by calling `sort_values` on the dataframe, passing `ascending=False`, and `set_index` on the `feature` column. (If you're having trouble with this step, try Googling the `'sort_values'` and `'set_index'` pandas functions, and if you're stuck for more than 5-10 minutes, ask your instructor or TA :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uniformity_Cell_Size</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Uniformity_Cell_Shape</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Single_Epithelial_Cell_Size</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Normal_Nucleoli</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mitoses</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marginal_Adhesion</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clump_Thickness</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bland_Chromatin</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bare_Nuclei</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature  importance\n",
       "1         Uniformity_Cell_Size        0.37\n",
       "2        Uniformity_Cell_Shape        0.23\n",
       "4  Single_Epithelial_Cell_Size        0.08\n",
       "7              Normal_Nucleoli        0.08\n",
       "8                      Mitoses        0.00\n",
       "3            Marginal_Adhesion        0.01\n",
       "0              Clump_Thickness        0.04\n",
       "6              Bland_Chromatin        0.13\n",
       "5                  Bare_Nuclei        0.05"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances.sort_values(by = 'feature', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `print` the `importances`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clump_Thickness</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uniformity_Cell_Size</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Uniformity_Cell_Shape</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marginal_Adhesion</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Single_Epithelial_Cell_Size</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bare_Nuclei</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bland_Chromatin</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Normal_Nucleoli</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mitoses</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature  importance\n",
       "0              Clump_Thickness        0.04\n",
       "1         Uniformity_Cell_Size        0.37\n",
       "2        Uniformity_Cell_Shape        0.23\n",
       "3            Marginal_Adhesion        0.01\n",
       "4  Single_Epithelial_Cell_Size        0.08\n",
       "5                  Bare_Nuclei        0.05\n",
       "6              Bland_Chromatin        0.13\n",
       "7              Normal_Nucleoli        0.08\n",
       "8                      Mitoses        0.00"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a list of features with their importances! Let's plot this in a very simple bar graph -- call `importances.plot.bar()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a151d77b8>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a15209b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the bar graph and discuss your observations with your partner. Which features are most important, and which features are least important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectFromModel\n",
    "\n",
    "`SelectFromModel` is quite simple -- it works by using a `threshold` to simply remove those features that are below the threshold. That's it! Let's do the following: use each of the feature importances that you printed above, as a threshold value. And for each one of those threshold values, create a `SelectFromModel` instance, call `fit` and `predict` on it, and find the `accuracy_score` for that prediction. What that means is that, we will be eliminating features one by one, and printing the accuracy each time.\n",
    "\n",
    "This part is pretty detailed, so we'll do it step by step.\n",
    "\n",
    "First, make the following imports:<br>\n",
    "`from numpy import sort\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sort\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a `RandomForestClassifier`, `fit` it to the training data, and call `predict` on the test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 4 4 4 4 2 2 4 2 4 2 2 4 2 2 4 4 4 4 2 2 4 2 4 2 4 2 2 2 4 4 2 4 4\n",
      " 2 4 4 2 2 4 4 2 4 2 4 4 4 4 4 4 4 2 4 2 2 4 2 4 4 2 2 4 2 2 4 4 2 2 4 2 2\n",
      " 4 2 2 4 2 4 4 2 4 4 2 4 4 2 2 2 2 2 4 4 2 4 2 2 2 4 2 2 2 2 2 4 2 4 4 2 2\n",
      " 4 2 2 2 4 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "predict = rfc.predict(X_test)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, call `accuracy_score` on your test data and your predictions. Print these scores using `np.round` and 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(accuracy_score(y_test, predict), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's see how the accuracy changes with different thresholds that we pass to `SelectFromModel`.\n",
    "\n",
    "    Create a `thresholds` array by calling `sort` on the `feature_importances_` of your RFC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00412472, 0.01162906, 0.02775226, 0.09461831, 0.10880304,\n",
       "       0.13676687, 0.14224745, 0.21526552, 0.25879277])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds = sort(rfc.feature_importances_)\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to iterate through the `thresholds` array in a `for` loop. In each loop, we will do the following:\n",
    "1. Call `SelectFromModel` on our model, passing the current threshold. Make sure to pass `prefit=True` since our model has been already fit on our training data (`X_train`, `y_train`).\n",
    "2. Transform the training features by calling `.transform` on your training data (`X_train`). Store results in `select_X_train`.\n",
    "3. Create a new `RandomForestClassifier` model, and call `.fit` on it using your transformed training features from step 2. above (`select_X_train`). Note that you can pass the original training labels `y_train` (because we are only transforming the features!)\n",
    "4. Transform our test features (`X_test`) as well by calling `.transform` on them. Store results in `select_X_test`.\n",
    "5. Call `.predict` on our model created in step 3., passing our transformed test features from step 4 (`select_X_test`). Store the results in `predictions`.\n",
    "6. Get the `accuracy_score` between our predictions from step 5 (`predictions`) and our original test labels (`y_test`).\n",
    "7. Print the following information:\n",
    "    * The current threshold\n",
    "    * The reduced number of features used (you can get this from `shape[1]` of `select_X_train`).\n",
    "    * The accuracy for this reduced number of features (you got this in step 6.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresh=0.004, n=9, Accuracy: 0.95\n",
      "Thresh=0.012, n=8, Accuracy: 0.93\n",
      "Thresh=0.028, n=7, Accuracy: 0.93\n",
      "Thresh=0.095, n=6, Accuracy: 0.95\n",
      "Thresh=0.109, n=5, Accuracy: 0.94\n",
      "Thresh=0.137, n=4, Accuracy: 0.93\n",
      "Thresh=0.142, n=3, Accuracy: 0.93\n",
      "Thresh=0.215, n=2, Accuracy: 0.91\n",
      "Thresh=0.259, n=1, Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    sfm = SelectFromModel(rfc, threshold=thresh, prefit=True)\n",
    "    select_X_train = sfm.transform(X_train)\n",
    "\n",
    "     # train model\n",
    "    rfc_new = RandomForestClassifier()\n",
    "    rfc_new.fit(select_X_train, y_train)\n",
    "\n",
    "    # eval model\n",
    "    select_X_test = sfm.transform(X_test)\n",
    "    predictions = rfc_new.predict(select_X_test).round(2)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f\" % (thresh, select_X_train.shape[1], accuracy))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False  True  True  True False False]\n"
     ]
    }
   ],
   "source": [
    "sfm_test =  SelectFromModel (rfc, threshold=0.095)\n",
    "sfm_test.fit(X_train, y_train)\n",
    "print(sfm_test.get_support()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? Here is an example of the kind of analysis that you could write for Assignment 2:\n",
    "\n",
    "*\"In general, we observe the performance of the model decreases with the number of selected features. Using RFC, the optimal model (trade-off) is obtained with only 4 features. A more complex model with 7 features would lead to similar accuracy, while a simpler model with only 2 features leads to a decrease in accuracy. These features are, in order of importance Uniformity_Cell_Shape, Single_Epithelial_Cell_Size, Uniformity_cell_Size and Bare_Nuclei.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination (or RFE) removes recursively features and builds a model on those features that remain. It uses the model estimated accuracy to identify which feature (or combination of features) contribute the most and rank them.\n",
    "\n",
    "Let's get a quick reminder of the features in order; print `df_features.columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Clump_Thickness', 'Uniformity_Cell_Size', 'Uniformity_Cell_Shape',\n",
       "       'Marginal_Adhesion', 'Single_Epithelial_Cell_Size', 'Bare_Nuclei',\n",
       "       'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, `from sklearn.feature_selection import RFE`, and create a new `RandomForestClassifier`. Call it `rfc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an `RFE` model and try the best `4` features. Create an instance of `RFE` and pass it your model (`rfc`), and `4` for the number of features to keep. Call your `RFE` model `rfe_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFE(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "  n_features_to_select=4, step=1, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "rfe_model= RFE(rfc, 4)\n",
    "print(rfe_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit `rfe_model` to your training data (`X_train`, `y_train`). Make sure to assign the model back to the fit! (\n",
    "`rfe_model = rfe_model.fit(....)`)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rfe_model = rfe_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print` the `support_` and `ranking_` attributes of your `rfe_model`.\n",
    "\n",
    "The `support_` attribute returns a boolean array of the feature indices that were chosen for your model from your original feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RFE' object has no attribute 'support_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-f54efaece7e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfe_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RFE' object has no attribute 'support_'"
     ]
    }
   ],
   "source": [
    "print(rfe_model.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ranking_` attribute shows you the rank for the features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfe_model.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate our new reduced feature set model on our test data! You can call `.predict` directly on your `rfe_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfe_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `print` the `accuracy_score` on your `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Again, what do the `support`, `ranking`, and the `accuracy score` suggest? Again, here's a short example of what you could write for Assignment 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The top 4 features to be selected (i.e. valued as 1) for RFC model are Uniformity_Cell_Shape, Uniformity_Cell_Size, Single_Epithelial_Cell_Size and Bare_Nuclei. The performance of this model counting only 4 features leads to an estimated test accuracy of (some value) %*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great job! Good luck on the Assignment, and let your instructor / TA know if you have any questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
