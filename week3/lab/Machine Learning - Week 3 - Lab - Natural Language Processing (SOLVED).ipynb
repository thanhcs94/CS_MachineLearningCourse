{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Lab - Natural Language Processing\n",
    "## Sentiment Analysis of Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Week 3 ! In today's lab, you will learn about Natural Language Processing (*NLP*). We will compare 3 methods of featurizing text data: \n",
    "* `CountVectorizer` (Bag of Words)\n",
    "* `TfidfVectorizer` (TF-IDF)\n",
    "* `Doc2Vec` \n",
    "\n",
    "in order to perform **sentiment analysis** on the Cornell IMDB movie review corpus (http://www.cs.cornell.edu/people/pabo/movie-review-data/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Format\n",
    "\n",
    "We can't directly input the raw reviews from the Cornell movie review data repository. Instead, we have to \"clean them up\" by:\n",
    "1. Converting everything to lower case\n",
    "2. Removing punctuation\n",
    "3. Removing common words (stop words)\n",
    "4. Stemming\n",
    "\n",
    "'Cleaning up' text is an important **Data Pre-processing** step in NLP, and is crucial to getting good results. In the same way that we do with our numerical features (egs: filling na values with a mean, etc.), we need to make sure that words that we are going to use as features are consistently formatted and don't include information that will end up being unnecessary.\n",
    "\n",
    "To practise, we are going to perform the above 4 steps on the sample movie review below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this searing parody of a students and teachers at a South London Public School leaves you literally rolling with laughter. It's vulgar, provocative, witty and sharp. The characters are a superbly caricatured cross section of British society (or to be more accurate, of any society). Following the escapades of Keisha, Latrina and Natella, our three \"protagonists\" for want of a better term, the show doesn't shy away from parodying every imaginable subject. Political correctness flies out the window in every episode. If you enjoy shows that aren't afraid to poke fun of every taboo subject imaginable, then Bromwell High will not disappoint!\n"
     ]
    }
   ],
   "source": [
    "movie_review = \"\"\"Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this searing parody of a students and teachers at a South London Public School leaves you literally rolling with laughter. It's vulgar, provocative, witty and sharp. The characters are a superbly caricatured cross section of British society (or to be more accurate, of any society). Following the escapades of Keisha, Latrina and Natella, our three \"protagonists\" for want of a better term, the show doesn't shy away from parodying every imaginable subject. Political correctness flies out the window in every episode. If you enjoy shows that aren't afraid to poke fun of every taboo subject imaginable, then Bromwell High will not disappoint!\"\"\"\n",
    "print(movie_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step is to lowercase it\n",
    "You can do this using the `.lower()` function. Try it out on `movie_review`, and print it to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review = movie_review.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to remove punctuation. import `string`, and then from `string` import `punctuation`.\n",
    "Print `punctuation` to see the list of punctuation marks in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we remove punctuation from a string is by creating a `translator` object, and then calling `.translate` on our string using the `translator` object.\n",
    "\n",
    "Create a `translator` object by calling `str.maketrans('', '', punctuation)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, call `.translate` on your `movie_review` and pass it your `translator` object. Then, print your movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwel high short brilliant expert script perfect deliv sear parodi student teacher south london public school leav liter roll laughter it vulgar provoc witti sharp the charact superbl caricatur cross section british societi accur societi follow escapad keisha latrina natella protagonist want better term doesnt shi away parodi imagin subject polit correct fli window episod if enjoy show arent afraid poke fun taboo subject imagin bromwel high disappoint'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review = movie_review.translate(translator)\n",
    "movie_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that all the punctuation has been removed!\n",
    "\n",
    "If you want to understand why/how this works, check out these posts:\n",
    "* https://www.tutorialspoint.com/python/string_maketrans.htm\n",
    "* https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words\n",
    "\n",
    "Notice all of the punctuation has been removed.  Next we will remove common words.  This is because in NLP we want to find things that distinct between different sets of texts.  We can make that easier by removing words that are common to ALL texts (and, is are, etc.)\n",
    "\n",
    "from `sklearn.feature_extraction.stop_words` import `ENGLISH_STOP_WORDS`. Then, print `ENGLISH_STOP_WORDS` to see a list of common stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove the above words from `movie_review`. First, convert `movie_review` into a list by calling the `.split()` method. Call your new object `split_review`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_review = movie_review.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now, you want to use a `for` loop to create a new list (call it `clean_words`). In each iteration of your loop, go through `split_review` and check every word. If the word is not in `ENGLISH_STOP_WORDS`, append it to your `clean_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = []\n",
    "for word in split_review:\n",
    "    if word not in ENGLISH_STOP_WORDS:\n",
    "        clean_words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Finally, put the clean words back together to re-create `movie_review`, by using the `.join'` method on `clean_words`, separated by a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_review  = \" \".join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your movie review! It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwel high short brilliant expert script perfect deliv sear parodi student teacher south london public school leav liter roll laughter vulgar provoc witti sharp charact superbl caricatur cross section british societi accur societi follow escapad keisha latrina natella protagonist want better term doesnt shi away parodi imagin subject polit correct fli window episod enjoy arent afraid poke fun taboo subject imagin bromwel high disappoint'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem words\n",
    "Finally, we will \"stem\" the words so that we take away the differences between words like \"expertly\" and \"expert\" since they have the same meaning. Read more on stemming [here](https://en.wikipedia.org/wiki/Stemming):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `SnowballStemmer` library. import it from `nltk.stem.snowball`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SnowballStemmer takes in a language as an argument. Since we are working with english, create a `SnowballStemmer` object and pass it `english` as the language. Call your object `stemmer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the stem of a word, call `stemmer.stem()`. Try it out with the word `running`. See what it prints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similar to how we removed the stop words, we want to now go through our review and stem each word. So:\n",
    "* Turn your `movie_review` back into a list using `.split()`\n",
    "* Create an empty list called `stemmed_words`\n",
    "* Use a `for` loop to go through every word in your `movie_review` and call `stemmer.stem` on it.\n",
    "* Append the newly stemmed word to your `stemmed_words` list\n",
    "* Finally, re-create movie_review into a string by calling `.join` using a space as your separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review = movie_review.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwel high short brilliant expert script perfect deliv sear parodi student teacher south london public school leav liter roll laughter it vulgar provoc witti sharp the charact superbl caricatur cross section british societi accur societi follow escapad keisha latrina natella protagonist want better term doesnt shi away parodi imagin subject polit correct fli window episod if enjoy show arent afraid poke fun taboo subject imagin bromwel high disappoint'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words = []\n",
    "for word in movie_review:\n",
    "    word = stemmer.stem(word)\n",
    "    stemmed_words.append(word)\n",
    "\n",
    "movie_review =  \" \".join(stemmed_words)    \n",
    "\n",
    "movie_review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your final movie review! It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwel high short brilliant expert script perfect deliv sear parodi student teacher south london public school leav liter roll laughter it vulgar provoc witti sharp the charact superbl caricatur cross section british societi accur societi follow escapad keisha latrina natella protagonist want better term doesnt shi away parodi imagin subject polit correct fli window episod if enjoy show arent afraid poke fun taboo subject imagin bromwel high disappoint'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put it all together\n",
    "We can put all the steps above together in a function, like this (pseudo-code given):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    clean_words = []\n",
    "    raw_text.lower()\n",
    "    raw_text = raw_text.translate(translator)\n",
    "    split_words = raw_text.split()\n",
    "    \n",
    "    for word in split_words:\n",
    "        if word not in ENGLISH_STOP_WORDS:\n",
    "            word = stemmer.stem(word)\n",
    "            clean_words.append(word)\n",
    "            \n",
    "    return ' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works. Here is an unclean review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this searing parody of a students and teachers at a South London Public School leaves you literally rolling with laughter. It's vulgar, provocative, witty and sharp. The characters are a superbly caricatured cross section of British society (or to be more accurate, of any society). Following the escapades of Keisha, Latrina and Natella, our three \"protagonists\" for want of a better term, the show doesn't shy away from parodying every imaginable subject. Political correctness flies out the window in every episode. If you enjoy shows that aren't afraid to poke fun of every taboo subject imaginable, then Bromwell High will not disappoint!\n"
     ]
    }
   ],
   "source": [
    "unclean_review = \"\"\"Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this searing parody of a students and teachers at a South London Public School leaves you literally rolling with laughter. It's vulgar, provocative, witty and sharp. The characters are a superbly caricatured cross section of British society (or to be more accurate, of any society). Following the escapades of Keisha, Latrina and Natella, our three \"protagonists\" for want of a better term, the show doesn't shy away from parodying every imaginable subject. Political correctness flies out the window in every episode. If you enjoy shows that aren't afraid to poke fun of every taboo subject imaginable, then Bromwell High will not disappoint!\"\"\"\n",
    "print(unclean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now clean it by calling the `clean_text` function above and make sure you notice the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwel high short brilliant expert script perfect deliv sear parodi student teacher south london public school leav liter roll laughter it vulgar provoc witti sharp the charact superbl caricatur cross section british societi accur societi follow escapad keisha latrina natella protagonist want better term doesnt shi away parodi imagin subject polit correct fli window episod if enjoy show arent afraid poke fun taboo subject imagin bromwel high disappoint'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_review = clean_text(unclean_review)\n",
    "clean_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now clean up all our data!\n",
    "\n",
    "Our data can be be found in the file `all_reviews_small.csv` (some of the cleaning steps have already been done)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import `pandas` and read `all_reviews_small.csv` into a dataframe. Call it `df_reviews`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_reviews = pd.read_csv('all_reviews_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the `head` and `shape`. You should see 4000 reviews, with 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>train_test_split</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>bromwell high is a cartoon comedy it ran at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>homelessness or houselessness as george carlin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>brilliant over acting by lesley ann warren bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>this is easily the most underrated film inn th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>this is not the typical mel brooks film it was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label train_test_split                                               text\n",
       "0   pos            train  bromwell high is a cartoon comedy it ran at th...\n",
       "1   pos            train  homelessness or houselessness as george carlin...\n",
       "2   pos            train  brilliant over acting by lesley ann warren bes...\n",
       "3   pos            train  this is easily the most underrated film inn th...\n",
       "4   pos            train  this is not the typical mel brooks film it was..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply our `clean_words` function to all the reviews! Store the clean reviews in a new column called `clean_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text =  df_reviews['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Check the `head` again to see your new dataframe's `clean_text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>train_test_split</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>bromwell high is a cartoon comedy it ran at th...</td>\n",
       "      <td>bromwel high cartoon comedi ran time program s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>homelessness or houselessness as george carlin...</td>\n",
       "      <td>homeless houseless georg carlin state issu yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>brilliant over acting by lesley ann warren bes...</td>\n",
       "      <td>brilliant act lesley ann warren best dramat ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>this is easily the most underrated film inn th...</td>\n",
       "      <td>easili underr film inn brook cannon sure flaw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>this is not the typical mel brooks film it was...</td>\n",
       "      <td>typic mel brook film slapstick movi actual plo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label train_test_split                                               text  \\\n",
       "0   pos            train  bromwell high is a cartoon comedy it ran at th...   \n",
       "1   pos            train  homelessness or houselessness as george carlin...   \n",
       "2   pos            train  brilliant over acting by lesley ann warren bes...   \n",
       "3   pos            train  this is easily the most underrated film inn th...   \n",
       "4   pos            train  this is not the typical mel brooks film it was...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  bromwel high cartoon comedi ran time program s...  \n",
       "1  homeless houseless georg carlin state issu yea...  \n",
       "2  brilliant act lesley ann warren best dramat ho...  \n",
       "3  easili underr film inn brook cannon sure flaw ...  \n",
       "4  typic mel brook film slapstick movi actual plo...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[\"clean_text\"] = clean_text\n",
    "df_reviews.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "In Python, the `CountVectorizer` object represents the Bag Of Words model. import it from `sklearn.feature_extraction.text`, and create a `CountVectorizer()` object called `count_vect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to convert all our `clean_text` reviews into a bag of words representation. Call `count_vect.fit_transform` on all our clean reviews (i.e. `df_review['clean_text']`) to do this. Save the result in `bag_of_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = count_vect.fit_transform(df_reviews['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your `bag_of_words`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9282)\t1\n",
      "  (0, 13792)\t1\n",
      "  (0, 6360)\t1\n",
      "  (0, 6212)\t1\n",
      "  (0, 18299)\t1\n",
      "  (0, 274)\t1\n",
      "  (0, 197)\t1\n",
      "  (0, 6052)\t1\n",
      "  (0, 20055)\t1\n",
      "  (0, 15629)\t1\n",
      "  (0, 9083)\t1\n",
      "  (0, 10530)\t1\n",
      "  (0, 3302)\t1\n",
      "  (0, 14825)\t1\n",
      "  (0, 8800)\t1\n",
      "  (0, 2450)\t1\n",
      "  (0, 18738)\t1\n",
      "  (0, 15061)\t1\n",
      "  (0, 5825)\t1\n",
      "  (0, 15812)\t1\n",
      "  (0, 9971)\t1\n",
      "  (0, 15018)\t1\n",
      "  (0, 16581)\t1\n",
      "  (0, 13613)\t1\n",
      "  (0, 13967)\t1\n",
      "  :\t:\n",
      "  (3999, 15667)\t1\n",
      "  (3999, 5893)\t1\n",
      "  (3999, 5340)\t1\n",
      "  (3999, 7405)\t1\n",
      "  (3999, 12832)\t3\n",
      "  (3999, 4802)\t3\n",
      "  (3999, 12062)\t1\n",
      "  (3999, 14781)\t1\n",
      "  (3999, 2099)\t1\n",
      "  (3999, 1047)\t1\n",
      "  (3999, 18820)\t1\n",
      "  (3999, 14436)\t1\n",
      "  (3999, 19988)\t1\n",
      "  (3999, 12045)\t1\n",
      "  (3999, 1713)\t1\n",
      "  (3999, 15818)\t1\n",
      "  (3999, 15862)\t1\n",
      "  (3999, 9091)\t1\n",
      "  (3999, 7499)\t1\n",
      "  (3999, 2989)\t1\n",
      "  (3999, 13841)\t1\n",
      "  (3999, 10496)\t1\n",
      "  (3999, 9654)\t1\n",
      "  (3999, 4794)\t2\n",
      "  (3999, 18299)\t1\n"
     ]
    }
   ],
   "source": [
    "bag_of_words\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our vocabulary is so large, CountVectorizer creates a sparse matrix for memory efficiency. Check `bag_of_words.shape`. You should see 4000 vectors, each with a dimension of 20719."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 20719)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bag_of_words is now a 4000 X 20,719 feature matrix, where every row is a move review, and every column is the count of words for the word that column represents. The words can be found using the `.get_feature_names()` method.\n",
    "\n",
    "Check the 200th word in `count_vect`. It should be `adulthood`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adulthood'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()[200] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can do the opposite using `.vocabulary_.get()`. Check `'adulthood'`; it should be the 200th word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(\"adulthood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "In Python, the `TfidfVectorizer` object represents the Bag Of Words model. import it from `sklearn.feature_extraction.text`, and create a `CountVectorizer()` object called `tf_idf_vect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, just like you did with the `CountVectorizer`, fit and transform your clean text reviews and store the result in a variable called `tf_idf`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2278)\t0.5855887807242065\n",
      "  (0, 8284)\t0.26592116063913257\n",
      "  (0, 2762)\t0.09029991322032968\n",
      "  (0, 3537)\t0.05638745903045433\n",
      "  (0, 14695)\t0.09457744785485278\n",
      "  (0, 18420)\t0.03230458413501131\n",
      "  (0, 14289)\t0.09160984007658156\n",
      "  (0, 15898)\t0.20455582137486983\n",
      "  (0, 10470)\t0.04733560734992249\n",
      "  (0, 18066)\t0.3611996528813187\n",
      "  (0, 20542)\t0.04433368983922523\n",
      "  (0, 18065)\t0.09061806545257328\n",
      "  (0, 14274)\t0.10997188049798773\n",
      "  (0, 10300)\t0.05682279347322916\n",
      "  (0, 1562)\t0.05092034552535194\n",
      "  (0, 15781)\t0.09498760381840703\n",
      "  (0, 3377)\t0.09817406573681878\n",
      "  (0, 14793)\t0.07758183927166384\n",
      "  (0, 15956)\t0.1276981011941443\n",
      "  (0, 17782)\t0.08588286428952507\n",
      "  (0, 6429)\t0.10558346176543791\n",
      "  (0, 9067)\t0.09061806545257328\n",
      "  (0, 17536)\t0.31275913685281154\n",
      "  (0, 15279)\t0.05369019100375045\n",
      "  (0, 13360)\t0.08767616862911443\n",
      "  :\t:\n",
      "  (3999, 19905)\t0.09164719069537093\n",
      "  (3999, 12051)\t0.11269137540016838\n",
      "  (3999, 18145)\t0.22003559744563847\n",
      "  (3999, 2181)\t0.08547048555848959\n",
      "  (3999, 8399)\t0.09390766756728264\n",
      "  (3999, 9101)\t0.10112937885503973\n",
      "  (3999, 16251)\t0.09867968550848664\n",
      "  (3999, 20030)\t0.1040720286472492\n",
      "  (3999, 14070)\t0.09563484663845614\n",
      "  (3999, 14343)\t0.1201821598027858\n",
      "  (3999, 10388)\t0.11596356879838926\n",
      "  (3999, 11260)\t0.09867968550848664\n",
      "  (3999, 1737)\t0.10775732185090751\n",
      "  (3999, 2627)\t0.09759296077094096\n",
      "  (3999, 7984)\t0.09311482187015728\n",
      "  (3999, 16167)\t0.1201821598027858\n",
      "  (3999, 15580)\t0.1040720286472492\n",
      "  (3999, 2384)\t0.1201821598027858\n",
      "  (3999, 14091)\t0.12612792987835583\n",
      "  (3999, 5970)\t0.12612792987835583\n",
      "  (3999, 11414)\t0.37838378963506747\n",
      "  (3999, 7988)\t0.12612792987835583\n",
      "  (3999, 10987)\t0.25225585975671166\n",
      "  (3999, 7662)\t0.12612792987835583\n",
      "  (3999, 16866)\t0.12612792987835583\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer()\n",
    "\n",
    "tf_idf = tf_idf_vect.fit_transform(df_reviews['clean_text'])\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `tf_idf` and notice how the values are different. Print the `shape` to confirm that the dimensions are the same as `bag_of_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 20719)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, because our dataset has so many unique words, tfidf vectorizer creates a sparse matrix.\n",
    "\n",
    "This matrix will again be 4000 X 20,719, where each column is the term frequence (count of times that word appears in the review) times the by the inverse document frequency (basically total number of reviews / number of reviews the word appears in)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the differences in the two feature sets for one of our reviews.\n",
    "\n",
    "Print the 9084th word in any one of our objects (`count_vect` or `tf_idf_vect`). It should be `inspir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inspir'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf_idf_vect.get_feature_names()[9084]\n",
    "count_vect.get_feature_names()[9084]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how often it appears in Review 1: print the value of `(1, 9084)` in your `bag_of_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words[1, 9084]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `1` !\n",
    "\n",
    "What about it's tf-idf value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04589627183322225"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[1, 9084]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `0.04589627183322225`.\n",
    "\n",
    "Notice how much smaller it is? This means it must appear in a good deal of other reviews\n",
    "\n",
    "### Classification\n",
    "\n",
    "Now, let's make a classifier to actually feed our feature data and train/test it. We'll use a Logistic Regression Classifier.\n",
    "\n",
    "First, do this for the `CountVectorizer`. Use `train_test_split` with `test_size=0.1` and `random_state=42`. Your features will simply be your `bag_of_words` and your labels will be `df_reviews['label']`. Because our data is balanced, you can use `accuracy_score` if you like to check the accuracy of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#LogisticRegression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.89      0.90      0.90       198\n",
      "        pos       0.90      0.90      0.90       202\n",
      "\n",
      "avg / total       0.90      0.90      0.90       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = bag_of_words\n",
    "y = df_reviews['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "model_loreg = LogisticRegression(solver='saga',multi_class='multinomial')\n",
    "model_loreg.fit(X_train, y_train)\n",
    "predict = model_loreg.predict(X_test)\n",
    "classifi_report = classification_report(y_test, predict)\n",
    "print(\"#LogisticRegression\")\n",
    "print(classifi_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "88.75 %, not bad! \n",
    "\n",
    "Now let's try using our `TfidfVectorizer` and see if it performs better. Use the same parameters as above, the only different is that your features are `tf_idf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#LogisticRegression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.90      0.91      0.91       198\n",
      "        pos       0.91      0.91      0.91       202\n",
      "\n",
      "avg / total       0.91      0.91      0.91       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = tf_idf\n",
    "y = df_reviews['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "model_loreg = LogisticRegression(solver='saga',multi_class='multinomial')\n",
    "model_loreg.fit(X_train, y_train)\n",
    "predict = model_loreg.predict(X_test)\n",
    "classifi_report = classification_report(y_test, predict)\n",
    "print(\"#LogisticRegression\")\n",
    "print(classifi_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90.25% ! So in this case, tf-idf is a little more accurate than bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Doc2Vec` documentation can be found here:<br>\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "A readable, easy introduction to `Doc2Vec` is available in this medium article:<br>\n",
    "https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "\n",
    "You don't need to understand the main details about how `Doc2Vec` works, but it's more important that you understand how to use it -- and *that* will be the goal of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First of all, you need to install `gensim`, which is the module that contains `Doc2Vec`. Open up your Terminal (on Mac) or Command Prompt (on Windows) and type in the following:\n",
    "\n",
    "`easy_install -U gensim`\n",
    "\n",
    "### Modules\n",
    "\n",
    "We use `gensim`, since `gensim` has a much more readable implementation of `Word2Vec` (and `Doc2Vec`). We also use `numpy` for general array manipulation, and `sklearn` for Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, from `gensim.models` import `Doc2Vec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import the usual suspects: `numpy`, and `LogisticRegression` from `sklearn.linear_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Doc2Vec model\n",
    "\n",
    "The way Doc2Vec works is that, each 'document' (lyrics of a song, words in an email, etc.) needs to be fully 'cleaned' (no punctuation, stemmed, etc.) and on a single line each in a `.txt` file. In our case, we have 50,000 movie reviews, split into 4 different `.txt` files:\n",
    "\n",
    "- `test-neg.txt`: 12500 negative movie reviews from the test data\n",
    "- `test-pos.txt`: 12500 positive movie reviews from the test data\n",
    "- `train-neg.txt`: 12500 negative movie reviews from the training data\n",
    "- `train-pos.txt`: 12500 positive movie reviews from the training data\n",
    "\n",
    "#### Check out the above text files and briefly go through them.\n",
    "\n",
    "You can look at the `Doc2VecHelperFunctions.ipynb` file if you are curious to see how the text files are converted into our Doc2Vec model, `imdb.d2v`.\n",
    "\n",
    "If you're curious about the parameters, do read the Doc2Vec/Word2Vec [documentation](\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, the model is already prepared. It is named `imdb.d2v`. Load it by calling `Doc2Vec.load('./imdb.d2v')` and save it in a variable called `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Model\n",
    "\n",
    "Let's see what our model gives. If we want to see what words are most 'similar' to `'good'`, we can call `model.vw.most_similar('good')` on our model. Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.7383522391319275),\n",
       " ('great', 0.7068538069725037),\n",
       " ('bad', 0.6740086078643799),\n",
       " ('fine', 0.6538172364234924),\n",
       " ('solid', 0.6522965431213379),\n",
       " ('nice', 0.6312327980995178),\n",
       " ('excellent', 0.5867269039154053),\n",
       " ('terrific', 0.5646074414253235),\n",
       " ('poor', 0.5573974847793579),\n",
       " ('strong', 0.5290021300315857)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are some of the words above used in similar ways in which you would use the word 'good' ? If yes, that means our model has kind of understood the *meaning* of the word `good`. This is really awesome (and important), since we are doing sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look deeper and see what the model actually contains. To see the feature vector for the first review in the training set for negative reviews, check `model['TRAIN_NEG_0']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f044ffbd76f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TRAIN_NEG_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model['TRAIN_NEG_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Sentiments\n",
    "\n",
    "### Training Vectors\n",
    "\n",
    "Now let's use these vectors to train a classifier. First, we must extract the training vectors. Remember that we have a total of 25000 training reviews, with equal numbers of positive and negative ones (12500 positive, 12500 negative). There are two parallel arrays, one containing the vectors (`train_arrays`) and the other containing the labels (`train_labels`). We simply put the positive ones at the first half of the array, and the negative ones at the second half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a `for` loop to go through all `25000` training reviews, adding the vector for each review in `train_arrays` and it's corresponding label (`1` for a positive review, and `0` for a negative review) in `train_labels`.\n",
    "\n",
    "#### Read the code below and ask your Instructor/TA if you have any questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays = np.zeros((25000, 100))\n",
    "train_labels = np.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[i] = model[prefix_train_pos]\n",
    "    train_arrays[12500 + i] = model[prefix_train_neg]\n",
    "    train_labels[i] = 1\n",
    "    train_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `train_arrays`. You should see rows and rows of vectors representing each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09523074,  0.10516991, -0.07066526, ..., -1.50765908,\n",
       "         0.37817046,  0.45435163],\n",
       "       [ 0.15591073, -1.00769353, -0.29605961, ..., -1.45913517,\n",
       "         1.49660051,  1.72079444],\n",
       "       [-0.49689472, -0.63923281, -1.31833351, ..., -2.12929225,\n",
       "         0.9443326 ,  0.63289094],\n",
       "       ...,\n",
       "       [-0.2536512 , -0.89831948, -0.24197805, ...,  1.50290143,\n",
       "         1.01230037, -0.3398996 ],\n",
       "       [-2.00854516,  0.64646685, -0.45022076, ...,  1.535079  ,\n",
       "         0.13337763,  0.06628666],\n",
       "       [-0.50857788,  0.85919422, -0.78979629, ..., -0.4446539 ,\n",
       "         1.05848455,  0.50058913]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `train_labels`. They are simply category labels for the sentence vectors -- 1 representing positive and 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Vectors\n",
    "\n",
    "We do the same for testing data -- data that we are going to feed to the classifier after we've trained it using the training data. This allows us to evaluate our results. The process is pretty much the same as extracting the results for the training data.\n",
    "\n",
    "#### Read the code below and ask your Instructor/TA if you have any questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = np.zeros((25000, 100))\n",
    "test_labels = np.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[i] = model[prefix_test_pos]\n",
    "    test_arrays[12500 + i] = model[prefix_test_neg]\n",
    "    test_labels[i] = 1\n",
    "    test_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Now, train a logistic regression classifier using the training data.\n",
    "\n",
    "Create a LogisticRegression Classifier, and `fit` it to your `train_arrays` and `train_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#LogisticRegression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.85      0.86      0.85      1202\n",
      "        1.0       0.87      0.86      0.86      1298\n",
      "\n",
      "avg / total       0.86      0.86      0.86      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = train_arrays\n",
    "y = train_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "model_loreg = LogisticRegression(solver='saga',multi_class='multinomial')\n",
    "model_loreg.fit(X_train, y_train)\n",
    "predict = model_loreg.predict(X_test)\n",
    "classifi_report = classification_report(y_test, predict)\n",
    "print(\"#LogisticRegression\")\n",
    "print(classifi_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `score` on your classifier, passing in your `test_arrays` and `test_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9075"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that we have achieved nearly 87% accuracy for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you have time, try running your classifier on a bunch of individual reviews and see if you agree with the predictions! You can do this in the following steps:\n",
    "* Choose a review from one of the `.txt` files.\n",
    "* You can grab it's corresponding vector by using the correct index in your model.\n",
    "    * For example, for the 3rd negative test review, the feature vector is `model['TEST_NEG_2']`\n",
    "* Call `classifier.predict` on your feature vector to see the prediction (you may have to use `.reshape` to get it in the correct format).\n",
    "    * A result of 1 means it's a positive review, and 0 means negative.\n",
    "* Do you agree :) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>once again mr costner has dragged out a movie for far longer than necessary aside from the terrific sea rescue sequences of which there are very few i just did not care about any of the characters most of us have ghosts in the closet and costner s character are realized early on and then forgotten until much later by which time i did not care the character we should really care about is a very cocky overconfident ashton kutcher the problem is he comes off as kid who thinks he s better than anyone else around him and shows no signs of a cluttered closet his only obstacle appears to be winning over costner finally when we are well past the half way point of this stinker costner tells us all about kutcher s ghosts we are told why kutcher is driven to be the best with no prior inkling or foreshadowing no magic here it was all i could do to keep from turning it off an hour in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is an example of why the majority of acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>first of all i hate those moronic rappers who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not even the beatles could write songs everyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brass pictures movies is not a fitting word fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a funny thing happened to me while watching mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  once again mr costner has dragged out a movie for far longer than necessary aside from the terrific sea rescue sequences of which there are very few i just did not care about any of the characters most of us have ghosts in the closet and costner s character are realized early on and then forgotten until much later by which time i did not care the character we should really care about is a very cocky overconfident ashton kutcher the problem is he comes off as kid who thinks he s better than anyone else around him and shows no signs of a cluttered closet his only obstacle appears to be winning over costner finally when we are well past the half way point of this stinker costner tells us all about kutcher s ghosts we are told why kutcher is driven to be the best with no prior inkling or foreshadowing no magic here it was all i could do to keep from turning it off an hour in \n",
       "0  this is an example of why the majority of acti...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "1  first of all i hate those moronic rappers who ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "2  not even the beatles could write songs everyon...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "3  brass pictures movies is not a fitting word fo...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "4  a funny thing happened to me while watching mo...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a random review\n",
    "data_review =  pd.read_csv('review_text_files/test-neg.txt')\n",
    "data_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a random review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a random review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a random review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a random review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, ask your Instructor or TA if you have any questions. Good luck on the Assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Doc2vec: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "- Paper that inspired this: https://arxiv.org/pdf/1405.4053.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
